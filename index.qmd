---
title: Moju Kapu モジュカプ
subtitle: How `mirai` and `crew` Are Powering the Next Generation of Parallel Computing in R
author: Charlie Gao and Will Landau
institute: Hibiki AI Limited, Eli Lilly and Company
date: July 9, 2024
format:
    revealjs:
        theme:
            - custom.scss
        incremental: true
        footer: "slides available at https://shikokuchuo.net/user2024-conference"
        embed-resources: true
        slide-number: true
editor:
    render-on-save: true
---

## {.center}

<style>
h1 {
  font-size: 1.6em !important;
}
h2 {
  font-size: 1.4em !important;
}
</style>

<img src="images/mojukapu.png" />

`moju-kapu` （モジュカプ） is shorthand for `modular encapsulation` （モジュラーカプセル化）

. . .

::: {.nonincremental}
- Effective stand-alone tools < > entire integrated systems
- Natural limits of a package
- Interfaces for developers as well as end-users
- Layered engineering approach
:::

## {.center}

Our Journey

- The missing piece in `targets`<br />
(Function-oriented Make-like declarative workflows for R)

- The genesis of `crew`<br />
(A distributed worker launcher)

- Collaborative development with `mirai`<br />
(Minimalist Async Evaluation Framework for R)

## {.center}

::: {.nonincremental}
A Brief Timeline

- Feb 2023 - CG/WL collaboration starts

- Mar 2023 - initial `mirai` backend for `crew`

- Apr 2023 - `targets` 1.0.0 with `crew` integration

- Jul 2023 - TLS lands in `mirai` and `crew`

- Oct 2023 - `mirai` implements parallel backend for R

- Dec 2023 - `mirai` serialization initial support for `torch`

- Mar 2024 - `mirai` supports ADBC database hosting

- May 2024 - `mirai` implements next-gen promises
:::

# mirai

## {.center}
::: {.nonincremental}

<img alt="mirai logo" src="images/mirai.png" width="120" />
mirai - the / moju kapu /
:::

. . . 

::: {.nonincremental}

* **Motivation**: production-grade parallel computing for R
* **Encapsulation**: developer interface for 3rd party launchers
* **Modularity**: compute profiles (modular by design)

:::

## {.center}

<img alt="mirai logo" src="images/mirai.png" width="120" />
mirai - [ Designed for Production ]

. . .

:::: {.columns}

::: {.column width="50%"}
::: {.nonincremental}
1. High Performance
2. Simple and Robust
3. Massively Scalable

:::
:::

::: {.column width="50%"}
::: {.nonincremental}
- 100x faster
- WYSIWYG concept
- one million promises

:::
:::
::::

## {.center}

mirai < Encapsulation >

[<img alt="R parallel" src="https://www.r-project.org/logo/Rlogo.png" width="40" />](https://shikokuchuo.net/mirai/articles/parallel.html) &nbsp; An alternative communications backend for R

[<img alt="promises" src="https://docs.posit.co/images/posit-ball.png" width="40" />](https://shikokuchuo.net/mirai/articles/promises.html) &nbsp; Next generation completely event-driven promises

[<img alt="Shiny" src="https://github.com/rstudio/shiny/raw/main/man/figures/logo.png" width="40" />](https://shikokuchuo.net/mirai/articles/shiny.html) &nbsp; Asynchronous parallel / distributed backend

[<img alt="Plumber" src="https://rstudio.github.io/cheatsheets/html/images/logo-plumber.png" width="40" />](https://shikokuchuo.net/mirai/articles/plumber.html) &nbsp; Asynchronous parallel / distributed backend

[<img alt="Arrow" src="https://arrow.apache.org/img/arrow-logo_hex_black-txt_white-bg.png" width="40" />](https://shikokuchuo.net/mirai/articles/databases.html) &nbsp; Host ADBC database connections in background processes

[<img alt="torch" src="https://torch.mlverse.org/css/images/hex/torch.png" width="40" />](https://shikokuchuo.net/mirai/articles/torch.html) &nbsp; Seamless cross-process use of Torch tensors and models

# crew

## Motivation for `crew`

<center>
<img src="./images/targets.png" width=200>
</center>

::: {.nonincremental style="font-size: 85%;"}

* `targets` is a pipeline tool for reproducible computation at scale in R.
* Before `crew`, `targets` struggled to:
    1. Scale out parallel workers to meet demand.
    2. Scale in parallel workers to conserve resources.
    3. Tailor itself to arbitrary distributed computing environments.

:::

## A `targets` pipeline

![](./images/autoscale1.png)

## A worker is an R process that runs tasks

![](./images/autoscale2.png)

## Add workers to meet demand

![](./images/autoscale3.png)

## Reuse workers for subsequent tasks

![](./images/autoscale4.png)


## Discard workers no longer needed

![](./images/autoscale5.png)

## Moju Kapu: how `mirai` supports `crew`

<center>
<img src="./images/crew-core.png" width=400>
</center>


::: {style="font-size: 85%"}

* `mirai` / `crew` integration:
    * `mirai` controls the tasks, and `crew` controls the workers.
    * Previously considered Redis to send tasks, but `mirai` is ideal.
* Moju kapu design of `crew`:
    * **Encapsulation**: centralized `R6` "controller" interface
    * **Modularity**: plugins for different computing environments

:::

## Encapsulation: `R6` classes to wrap `mirai`

:::: {.columns  style="font-size: 90%;"}

::: {.column width="45%"}

<br>

<center>
<img src="./images/crew-design.png" width=400>
</center>

:::

::: {.column width="55%"}

<br>

```{r, eval = FALSE, echo = TRUE}
# Start a new controller.
x <- crew::crew_controller_local(
  workers = 10,
  seconds_idle = 30
)

# Submit many parallel tasks.
x$walk(
  rnorm(1, x),
  iterate = list(x = seq_len(1000))
)

# Optional: wait for all tasks.
x$wait(mode = "all")

# Collect results so far.
str(unlist(x$collect()$result))
#> num [1:1000] 3.2 4.1 2.31 ...
```

:::

::::


## Modularity: `crew` plugins

:::: {.columns  style="font-size: 70%;"}

::: {.column width="50%"}


<center>
<img src="./images/crew.png" width=150>
</center>

```{r, echo = TRUE, eval = FALSE}
crew_controller_local()
```

<br>

<center>
<img src="./images/crew.cluster.png" width=150>
</center>

```{r, echo = TRUE, eval = FALSE}
crew_controller_slurm(
  slurm_memory_gigabytes_per_cpu = 16,
  script_lines = "module load R/4.4.0"
)
```

:::

::: {.column width="50%"}

<center>
<img src="./images/crew.aws.batch.png" width=150>
</center>

```{r, echo = TRUE, eval = FALSE}
crew_controller_aws_batch(
  aws_batch_job_definition = "your_def",
  aws_batch_job_queue = "your_queue"
)
```

<center>
<img src="./images/hex-custom.png" width=150>
</center>

```{r, echo = TRUE, eval = FALSE}
your_custom_controller(...)
```

:::

::::



## Users can write `crew` plugins

::: {.nonincremental}

```{r, echo = TRUE, eval = FALSE}
custom_launcher_class <- R6::R6Class(
  classname = "custom_launcher_class",
  inherit = crew::crew_class_launcher,
  public = list(
    launch_worker = function(call, name, launcher, worker, instance) {

      # 1. Reserve compute for R to run, e.g. start a job on a cluster.
      # 2. Make that job start an R process.
      # 3. Make that R process run the code in `call`.

    },
    terminate_worker = function(handle) {

      # Terminate a worker.

    }
  )
)
```

* How to write a `crew` plugin: <https://wlandau.github.io/crew/articles/plugins.html>

:::

## `targets` accepts any `crew` controller

::: {.nonincremental}

:::: {.columns}

::: {.column width="20%"}

<center>
<img src="./images/targets.png">
</center>

:::

::: {.column width="80%"}


<center>
<img src="./images/graph.png">
</center>

:::

::::

```{r, eval = FALSE, echo = TRUE}
tar_option_set(
  controller = crew_controller_aws_batch(
    workers = 3,
    seconds_idle = 60,
    aws_batch_job_definition = "your_def",
    aws_batch_job_queue = "your_queue",
    aws_batch_region = "us-east-2"
  )
)
```

* <https://books.ropensci.org/targets/crew.html>

:::

# Appendix

# 1. mirai Design Concepts

## 100x Faster

Setup:

``` r
library(mirai)
library(future)

d <- daemons(1, dispatcher = FALSE)
plan("multisession", workers = 1)

m <- mirai(1)
collect_mirai(m)
#> [1] 1

f <- future(1)
value(f)
#> [1] 1
```

<sup>Created on 2024-05-27 with [reprex v2.1.0](https://reprex.tidyverse.org)</sup>

## 100x Faster (Cont'd)

Benchmarking:

``` r
bench::mark(mirai(1), future(1), relative = TRUE, check = FALSE)
#> # A tibble: 2 × 6
#>   expression   min median `itr/sec` mem_alloc `gc/sec`
#>   <bch:expr> <dbl>  <dbl>     <dbl>     <dbl>    <dbl>
#> 1 mirai(1)      1      1       74.6      1        1   
#> 2 future(1)   158.   113.       1        5.72     2.75

bench::mark(collect_mirai(m), value(f), relative = TRUE)
#> # A tibble: 2 × 6
#>   expression         min median `itr/sec` mem_alloc `gc/sec`
#>   <bch:expr>       <dbl>  <dbl>     <dbl>     <dbl>    <dbl>
#> 1 collect_mirai(m)   1      1        84.1       Inf      NaN
#> 2 value(f)          79.3   89.0       1         NaN      Inf
```

<sup>Created on 2024-05-27 with [reprex v2.1.0](https://reprex.tidyverse.org)</sup>

## WYSIWYG Concept

Production usage requires 'correctness' over 'convenience'.

No reliance on non-transparent static code analysis, which can be inefficient, or fail due to hidden global options:

``` r
library(mirai)
library(future)
df <- list(a = double(1e8), b = 1)

m <- mirai(2 * x, x = df$b)
m[]
#> [1] 2

f <- future(2 * df$b)
#> Error in getGlobalsAndPackages(expr, envir = envir, tweak =
#> tweakExpression, : The total size of the 1 globals exported for
#> future expression ('2 * df$b') is 762.94 MiB.. This exceeds the
#> maximum allowed size of 500.00 MiB (option 'future.globals.maxSize')
#> . There is one global: 'df' (762.94 MiB of class 'list')
```
## One Million Promises

``` r
library(mirai)
daemons(8, dispatcher = FALSE)
#> [1] 8

r <- 0
start <- Sys.time()
m <- mirai_map(1:1000000, \(x) x, .promise = \(x) r <<- r + x)
Sys.time() - start
#> Time difference of 6.42396 mins

later::run_now()
r
#> [1] 500000500000
```

<sup>Created on 2024-05-27 with [reprex v2.1.0](https://reprex.tidyverse.org) <br />
Running on an Intel i7 Gen 11 notebook with 16GB RAM.</sup>

