---
title: Moju Kapu モジュカプ
subtitle: How `mirai` and `crew` Are Powering the Next Generation of Parallel Computing in R
author: Charlie Gao and Will Landau
institute: Hibiki AI Limited, Eli Lilly and Company
date: July 9, 2024
format:
    revealjs:
        theme:
            - custom.scss
        incremental: true
        footer: "slides available at https://shikokuchuo.net/user2024-conference"
        embed-resources: true
        slide-number: true
editor:
    render-on-save: true
---

## {.center}

<style>
h1 {
  font-size: 1.6em !important;
}
h2 {
  font-size: 1.4em !important;
}
</style>

<img src="images/mojukapu.png" />

`moju-kapu` （モジュカプ） is shorthand for `modular encapsulation` （モジュラーカプセル化）

. . .

::: {.nonincremental}
- Effective stand-alone tools < > entire integrated systems
- Natural limits of a package
- Interfaces for developers as well as end-users
- Layered engineering approach
:::

## {.center}

Our Journey

- The missing piece in `targets`<br />
(Function-oriented Make-like declarative workflows for R)

- The genesis of `crew`<br />
(A distributed worker launcher)

- Collaborative development with `mirai`<br />
(Minimalist Async Evaluation Framework for R)

# mirai

## {.center}
::: {.nonincremental}

<img alt="mirai logo" src="images/mirai.png" width="120" />
mirai - the / moju kapu /
:::

. . . 

::: {.nonincremental}

* **Motivation**: production-grade parallel computing for R
* **Modularity**: interfaces with and enhances top packages
* **Encapsulation**: developer interface for 3rd party launchers

:::

# mirai: motivation

##

<img alt="mirai logo" src="images/mirai.png" width="120" />
mirai - [ Designed for Production ]

. . .

:::: {.columns}

::: {.column width="50%"}
::: {.nonincremental}
1. High Performance

:::
:::

::: {.column width="50%"}
::: {.nonincremental}
- 100x faster

:::
:::
::::

## 100x Faster

Benchmarking:


```{.r code-line-numbers="|6|13|"}
bench::mark(mirai(1), future(1), relative = TRUE, check = FALSE)
#> # A tibble: 2 × 6
#>   expression   min median `itr/sec` mem_alloc `gc/sec`
#>   <bch:expr> <dbl>  <dbl>     <dbl>     <dbl>    <dbl>
#> 1 mirai(1)      1      1       74.6      1        1   
#> 2 future(1)   158.   113.       1        5.72     2.75

bench::mark(collect_mirai(m), value(f), relative = TRUE)
#> # A tibble: 2 × 6
#>   expression         min median `itr/sec` mem_alloc `gc/sec`
#>   <bch:expr>       <dbl>  <dbl>     <dbl>     <dbl>    <dbl>
#> 1 collect_mirai(m)   1      1        84.1       Inf      NaN
#> 2 value(f)          79.3   89.0       1         NaN      Inf
```

<sup>Created on 2024-05-27 with [reprex v2.1.0](https://reprex.tidyverse.org)</sup>

##

<img alt="mirai logo" src="images/mirai.png" width="120" />
mirai - [ Designed for Production ]

:::: {.columns}

::: {.column width="50%"}
::: {.nonincremental}
1. High Performance
2. Simple and Robust

:::
:::

::: {.column width="50%"}
::: {.nonincremental}
- 100x faster
- WYSIWYG concept

:::
:::
::::

## WYSIWYG Concept

Production usage requires 'correctness' over 'convenience'

Compare and contrast:

:::: {.columns}
::: {.column width="50%"}

``` r
library(mirai)

d <- list(a = double(1e8), b = 1)

m <- mirai(2 * x, x = d$b)
m[]
#> [1] 2
```
:::

::: {.column width="50%"}

``` r
library(future)

d <- list(a = double(1e8), b = 1)

f <- future(2 * d$b)
#> Error in getGlobalsAndPackages(
#> expr, envir = envir, tweak =
#> tweakExpression, : The total
#> size of the 1 globals exported
#> for future expression ('2 *
#> d$b') is 762.94 MiB.. This
#> exceeds the maximum allowed
#> size of 500.00 MiB (option
#> 'future.globals.maxSize').
#> There is one global: 'd'
#> (762.94 MiB of class 'list')
```

:::
::::

## WYSIWYG Concept (Cont'd)

`mirai_map()` does not use 'chunking':

```{.r code-line-numbers="|1-4|6|8-10|12-14|16-18|"}
library(mirai)
library(parallel)
cl <- make_cluster(4)
daemons(4)

vec <- c(1, 1, 4, 4, 1, 1, 1, 1)

system.time(mirai_map(vec, Sys.sleep)[])
#>    user  system elapsed 
#>   0.005   0.002   4.006

system.time(parLapply(cl, vec, Sys.sleep))
#>    user  system elapsed 
#>   0.008   0.073   8.075

system.time(parLapplyLB(cl, vec, Sys.sleep))
#>    user  system elapsed 
#>   0.026   0.063   5.084
```

##

<img alt="mirai logo" src="images/mirai.png" width="120" />
mirai - [ Designed for Production ]

:::: {.columns}

::: {.column width="50%"}
::: {.nonincremental}
1. High Performance
2. Simple and Robust
3. Massively Scalable

:::
:::

::: {.column width="50%"}
::: {.nonincremental}
- 100x faster
- WYSIWYG concept
- one million promises

:::
:::
::::

## One Million Promises

```{.r code-line-numbers="|1-3|5-9|11-13|"}
library(mirai)
daemons(8, dispatcher = FALSE)
#> [1] 8

r <- 0
start <- Sys.time()
m <- mirai_map(1:1000000, \(x) x, .promise = \(x) r <<- r + x)
Sys.time() - start
#> Time difference of 6.42396 mins

later::run_now()
r
#> [1] 500000500000
```

<sup>Created on 2024-05-27 with [reprex v2.1.0](https://reprex.tidyverse.org) <br />
Running on an Intel i7 Gen 11 notebook with 16GB RAM.</sup>

##

<img alt="mirai logo" src="images/mirai.png" width="120" />
mirai - [ Designed for Production ]

:::: {.columns}

::: {.column width="50%"}
::: {.nonincremental}
1. High Performance
2. Simple and Robust
3. Massively Scalable

:::
:::

::: {.column width="50%"}
::: {.nonincremental}
- 100x faster
- WYSIWYG concept
- one million promises

:::
:::
::::

# mirai: modularity

## Interfaces with and Enhances

[<img alt="R parallel" src="https://www.r-project.org/logo/Rlogo.png" width="40" />](https://shikokuchuo.net/mirai/articles/parallel.html) &nbsp; An alternative communications backend for R

[<img alt="promises" src="https://docs.posit.co/images/posit-ball.png" width="40" />](https://shikokuchuo.net/mirai/articles/promises.html) &nbsp; Next generation completely event-driven promises

[<img alt="Shiny" src="https://github.com/rstudio/shiny/raw/main/man/figures/logo.png" width="40" />](https://shikokuchuo.net/mirai/articles/shiny.html) &nbsp; Asynchronous parallel / distributed backend

[<img alt="Plumber" src="https://rstudio.github.io/cheatsheets/html/images/logo-plumber.png" width="40" />](https://shikokuchuo.net/mirai/articles/plumber.html) &nbsp; Asynchronous parallel / distributed backend

[<img alt="Arrow" src="https://arrow.apache.org/img/arrow-logo_hex_black-txt_white-bg.png" width="40" />](https://shikokuchuo.net/mirai/articles/databases.html) &nbsp; Host ADBC database connections in background processes

[<img alt="torch" src="https://torch.mlverse.org/css/images/hex/torch.png" width="40" />](https://shikokuchuo.net/mirai/articles/torch.html) &nbsp; Seamless cross-process use of Torch tensors and models

# mirai: encapsulation

## Motivation for `crew`

<center>
<img src="./images/targets.png" width=200>
</center>

::: {.nonincremental style="font-size: 85%;"}

* `targets` is a pipeline tool for reproducible computation at scale in R
* Manages large workflows in statistics and data science:
    * Bayesian data analysis
    * Machine learning
    * Simulation of clinical trials
    * Genomic data analysis

:::

## Challenges

<center>
<img src="./images/targets.png" width=200>
</center>

::: {.nonincremental style="font-size: 85%;"}

1. Scale out parallel workers to meet demand
2. Scale in parallel workers to conserve resources
3. Tailor itself to arbitrary distributed computing environments

:::

## A `targets` pipeline

![](./images/autoscale1.png)

## A worker is an R process that runs tasks

![](./images/autoscale2.png)

## Add workers to meet demand

![](./images/autoscale3.png)

## Reuse workers for subsequent tasks

![](./images/autoscale4.png)


## Discard workers no longer needed

![](./images/autoscale5.png)

## Beginnings: `mirai` and `crew`

<center>
<img src="./images/crew-core.png" width=300>
</center>

::: {style="font-size:85%"}

* `crew` needed a backend to communicate with parallel workers over a local network connection
* Originally considered Redis, but `mirai` is ideal:
    * Does not depend on Redis server
    * Can send larger data objects over the network
* `crew` began using `mirai` in February 2023

:::

## How `mirai` supports `crew`

<center>
<img src="./images/crew-core.png" width=300>
</center>

::: {style="font-size:85%"}

* `crew` launches workers, `mirai` sends tasks to workers
* `mirai` supports modular building blocks for `crew`:
   * **`mirai::daemon(url = "...")`: turn any R process into a worker on the network.**
   * `mirai::saisei()`: rotates websocket connections
   * Down-scaling workers: maximum idle time, maximum number of tasks

:::

## Moju Kapu design of `crew`

<center>
<img src="./images/crew-core.png" width=300>
</center>

* **Encapsulation**: centralized `R6` "controller" interface
* **Modularity**: plugins for different computing environments

## Encapsulation: `R6` classes to wrap `mirai`

:::: {.columns  style="font-size: 90%;"}

::: {.column width="45%"}

<br>

<center>
<img src="./images/crew-design.png" width=400>
</center>

:::

::: {.column width="55%"}

<br>

```{r, eval = FALSE, echo = TRUE}
# Start a new controller.
x <- crew::crew_controller_local(
  workers = 10,
  seconds_idle = 30
)

# Submit many parallel tasks.
x$walk(
  rnorm(1, x),
  iterate = list(x = seq_len(1000))
)

# Optional: wait for all tasks.
x$wait(mode = "all")

# Collect results so far.
str(unlist(x$collect()$result))
#> num [1:1000] 3.2 4.1 2.31 ...
```

:::

::::


## Modularity: `crew` plugins

:::: {.columns  style="font-size: 70%;"}

::: {.column width="50%"}


<center>
<img src="./images/crew.png" width=150>
</center>

```{r, echo = TRUE, eval = FALSE}
crew_controller_local()
```

<br>

<center>
<img src="./images/crew.cluster.png" width=150>
</center>

```{r, echo = TRUE, eval = FALSE}
crew_controller_slurm(
  slurm_memory_gigabytes_per_cpu = 16,
  script_lines = "module load R/4.4.0"
)
```

:::

::: {.column width="50%"}

<center>
<img src="./images/crew.aws.batch.png" width=150>
</center>

```{r, echo = TRUE, eval = FALSE}
crew_controller_aws_batch(
  aws_batch_job_definition = "your_def",
  aws_batch_job_queue = "your_queue"
)
```

<center>
<img src="./images/hex-custom.png" width=150>
</center>

```{r, echo = TRUE, eval = FALSE}
your_custom_controller(...)
```

:::

::::

## Users can write `crew` plugins

::: {.nonincremental}

```{r, echo = TRUE, eval = FALSE}
custom_launcher_class <- R6::R6Class(
  classname = "custom_launcher_class",
  inherit = crew::crew_class_launcher,
  public = list(
    launch_worker = function(call, name, launcher, worker, instance) {

      # 1. Reserve compute for R to run, e.g. start a job on a cluster.
      # 2. Make that job start an R process.
      # 3. Make that R process run the code in `call`.

    },
    terminate_worker = function(handle) {

      # Terminate a worker.

    }
  )
)
```

* How to write a `crew` plugin: <br />
<https://wlandau.github.io/crew/articles/plugins.html>

:::

## `targets` accepts any `crew` controller

::: {.nonincremental}

:::: {.columns}

::: {.column width="20%"}

<center>
<img src="./images/targets.png">
</center>

:::

::: {.column width="80%"}


<center>
<img src="./images/graph.png">
</center>

:::

::::

```{r, eval = FALSE, echo = TRUE}
tar_option_set(
  controller = crew_controller_aws_batch(
    workers = 3,
    seconds_idle = 60,
    aws_batch_job_definition = "your_def",
    aws_batch_job_queue = "your_queue",
    aws_batch_region = "us-east-2"
  )
)
```

* <https://books.ropensci.org/targets/crew.html>

:::

